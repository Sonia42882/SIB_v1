{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho SIB\n",
    "\n",
    "Trabalho realizado no âmbito da unidade curricular de Sistemas Inteligentes para a Bioinformática.\n",
    "\n",
    "Grupo constituído por:\n",
    "\n",
    "    - Angelina Eiras PG42861\n",
    "    - Carina Gonçalves PG45466\n",
    "    - Rute Castro PG45475\n",
    "    - Sónia Carvalho PG42882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/jcapels/propythia.git@fix_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\asus\\onedrive - universidade do minho\\carina\\sib\\propythia.zipNote: you may need to restart the kernel to use updated packages.\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (1.21.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (1.7.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (1.4.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (3.5.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (0.0.post1)\n",
      "Requirement already satisfied: biopython in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (1.80)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (2.11.0)\n",
      "Requirement already satisfied: Keras in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (2.11.0)\n",
      "Requirement already satisfied: umap in c:\\users\\asus\\anaconda3\\lib\\site-packages (from propythia==2.0.0) (0.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->propythia==2.0.0) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->propythia==2.0.0) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->propythia==2.0.0) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->propythia==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->propythia==2.0.0) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->propythia==2.0.0) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib->propythia==2.0.0) (9.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->propythia==2.0.0) (2021.3)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->propythia==2.0.0) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (4.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.28.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.42.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (3.19.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (22.11.23)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (3.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.33.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow->propythia==2.0.0) (3.2.2)\n",
      "Building wheels for collected packages: propythia\n",
      "  Building wheel for propythia (setup.py): started\n",
      "  Building wheel for propythia (setup.py): finished with status 'done'\n",
      "  Created wheel for propythia: filename=propythia-2.0.0-py3-none-any.whl size=369524 sha256=a70077bf3af56aba5cb3dcd775143c4f8b354cba85e0f672bfce46a44bb1779e\n",
      "  Stored in directory: C:\\Users\\Asus\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-u0atq2g2\\wheels\\ce\\76\\29\\a6b30aaccc44618c669c914d1552f0cac041aefcb6286f7806\n",
      "Successfully built propythia\n",
      "Installing collected packages: propythia\n",
      "  Attempting uninstall: propythia\n",
      "    Found existing installation: propythia 2.0.0\n",
      "    Uninstalling propythia-2.0.0:\n",
      "      Successfully uninstalled propythia-2.0.0\n",
      "Successfully installed propythia-2.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install propythia.zip --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from seaborn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install umap-learn[plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bio-embeddings[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secção 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploração inicial e pré-processamento**\n",
    "\n",
    "• Rever toda a documentação disponível sobre o conjunto de dados.\n",
    "\n",
    "• Carregar o conjunto de dados e realizar uma análise exploratória do mesmo\n",
    "\n",
    "• Realizar os passos necessários de preparação dos dados e pré-processamento, incluindo possivelmente a geração de atributos, a sua seleção, o tratamento de possíveis valores em falta, etc.\n",
    "\n",
    "**Esta etapa deve corresponder à secção 1 do Notebook onde deverá:**\n",
    "\n",
    "• descrever e caracterizar os dados atribuídos de acordo com a documentação/literatura existente;\n",
    "\n",
    "• descrever sucintamente as características dos dados disponíveis a partir da análise exploratória inicial;\n",
    "\n",
    "• descrever os passos de preparação dos dados e pré-processamento que efetuou, justificando as suas escolhas;\n",
    "\n",
    "• incluir os gráficos exploratórios iniciais que ilustrem as principais características dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant' (C:\\Users\\sonia\\anaconda3\\envs\\SIB\\lib\\site-packages\\charset_normalizer\\constant.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIB\\lib\\site-packages\\requests\\compat.py:11\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mr\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mBio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SeqIO\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StringIO\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIB\\lib\\site-packages\\requests\\__init__.py:45\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib3\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RequestsDependencyWarning\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m charset_normalizer_version\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIB\\lib\\site-packages\\requests\\exceptions.py:9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mrequests.exceptions\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;03mThis module contains the set of Requests' exceptions.\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m BaseHTTPError\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m CompatJSONDecodeError\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mRequestException\u001B[39;00m(\u001B[38;5;167;01mIOError\u001B[39;00m):\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;124;03m    request.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIB\\lib\\site-packages\\requests\\compat.py:13\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m---> 13\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# -------\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Pythons\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# -------\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Syntax sugar.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIB\\lib\\site-packages\\charset_normalizer\\__init__.py:23\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mCharset-Normalizer\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03m:license: MIT, see LICENSE for more details.\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_fp, from_path, from_bytes, normalize\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m detect\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__, VERSION\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIB\\lib\\site-packages\\charset_normalizer\\api.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m     PathLike \u001B[38;5;241m=\u001B[39m Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mos.PathLike[str]\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstant\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mess_ratio\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharsetMatches, CharsetMatch\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m warn\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SIB\\lib\\site-packages\\charset_normalizer\\md.py:5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Optional, List\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstant\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m UNICODE_SECONDARY_RANGE_KEYWORD\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_punctuation, is_symbol, unicode_range, is_accentuated, is_latin, \\\n\u001B[0;32m      6\u001B[0m     remove_accent, is_separator, is_cjk, is_case_variable, is_hangul, is_katakana, is_hiragana, is_ascii, is_thai\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMessDetectorPlugin\u001B[39;00m:\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124;03m    Base abstract class used for mess detection plugins.\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m    All detectors MUST extend and implement given methods.\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant' (C:\\Users\\sonia\\anaconda3\\envs\\SIB\\lib\\site-packages\\charset_normalizer\\constant.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import requests as r\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from keras.optimizers.schedules import *\n",
    "\n",
    "from propythia import *\n",
    "from propythia.protein_descriptores import ProteinDescritors\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import auc, roc_curve, matthews_corrcoef, f1_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", delimiter=';')\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_update = pd.read_csv(\"train_updates_20220929.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#update dos numeros de seq_id no df inicial\n",
    "for i in df_update['seq_id']:\n",
    "    df[df['seq_id'] == i] = df_update[df_update['seq_id'] == i]\n",
    "\n",
    "#nao precisamos dos links\n",
    "df.drop(['data_source'], axis=1, inplace=True)\n",
    "df_test.drop(['data_source'], axis=1, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.info(verbose=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.optimizers.schedules import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from propythia import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# gerar descritores\n",
    "\n",
    "descriptors_df = ProteinDescritors(dataset= df ,  col= 'protein_sequence')\n",
    "descriptors_df.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "descriptors_df.get_lenght(n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "descriptors_df.get_aa_comp(n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## NAO CORRER - É O QUE VAMOS USAR\n",
    "## COMO DEMORA MUITO, METI NUM EXCEL E É SÓ IMPORTAR ESSE PARA TERMOS O DATASET DIREITO\n",
    "\n",
    "#all_descriptors = descriptors_df.get_all_physicochemical(ph=7, amide=False, n_jobs=4)\n",
    "#all_descriptors\n",
    "#all_descriptors.to_csv('descriptors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar dados fisico quimicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"descriptors.csv\", delimiter=';')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "só correr uma vez !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop das colunas que só têm valores = 0\n",
    "\n",
    "data = dataset.drop(columns=['tot', 'hydrogen', 'single', 'double'], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separar o x do y e normalizar os dados entre 0 e 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 3:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# normalização dos dados entre zero e 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_norm = scaler.fit_transform(X)\n",
    "data_norm\n",
    "\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision = 3)\n",
    "print(data_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalizar com standardscaler -- ver qual é melhor!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_st = StandardScaler().fit(X)\n",
    "data_st = scaler_st.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision = 3)\n",
    "print(data_st[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = X.columns\n",
    "colname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA com dados iniciais (sem transformação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"green\" if active else \"red\" for active in y]\n",
    "\n",
    "principalComponents = PCA(n_components=2).fit_transform(X)\n",
    "xs = principalComponents[:,0]\n",
    "ys = principalComponents[:,1]\n",
    "\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(xs)): axe.plot(xs[j], ys[j], marker=\"o\", color=colors[j], markeredgecolor=\"dimgrey\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA com transformação MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"green\" if active else \"red\" for active in y]\n",
    "# blue is 1 green is 0\n",
    "\n",
    "principalComponents = PCA().fit_transform(data_norm)\n",
    "xs = principalComponents[:,0]\n",
    "ys = principalComponents[:,1]\n",
    "\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(xs)): axe.plot(xs[j], ys[j], marker=\"o\", color=colors[j], markeredgecolor=\"dimgrey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA com transformação StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"green\" if active else \"red\" for active in y]\n",
    "\n",
    "\n",
    "principalComponents = PCA().fit_transform(data_st)\n",
    "xs = principalComponents[:,0]\n",
    "ys = principalComponents[:,1]\n",
    "\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(xs)): axe.plot(xs[j], ys[j], marker=\"o\", color=colors[j], markeredgecolor=\"dimgrey\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE com dados iniciais (sem transformação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = TSNE(n_components=2,n_iter=1000).fit_transform(X)\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(embeded)): axe.plot(embeded[j,0],embeded[j,1],marker=\"o\",color=colors[j],markeredgecolor=\"dimgrey\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE com transformação MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = TSNE(n_components=2,n_iter=1000).fit_transform(data_norm)\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(embeded)): axe.plot(embeded[j,0],embeded[j,1],marker=\"o\",color=colors[j],markeredgecolor=\"dimgrey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE com transformação StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = TSNE(n_components=2,n_iter=1000).fit_transform(data_st)\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(embeded)): axe.plot(embeded[j,0],embeded[j,1],marker=\"o\",color=colors[j],markeredgecolor=\"dimgrey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectPercentile com os dados entre 0 e 1 --- se quiseres StandardScaler -- mudar o 'data_norm' para 'data_st'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Percentile\n",
    "feature_selection_percentile = SelectPercentile(chi2, percentile=25)\n",
    "feature_selection_percentile.fit(data_norm, y)\n",
    "\n",
    "selected_features_percentile = X.columns[feature_selection_percentile.get_support()]\n",
    "selected_features_percentile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_percentile = selected_features_percentile\n",
    "\n",
    "lista_percentile = []\n",
    "for i in method_percentile:\n",
    "    lista_percentile.append(i)\n",
    "lista_percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectKBest com os dados entre 0 e 1 --- se quiseres StandardScaler -- mudar o 'data_norm' para 'data_st'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectKBest\n",
    "feature_selection_kbest = SelectKBest(chi2, k=25)\n",
    "feature_selection_kbest.fit(data_norm, y)\n",
    "\n",
    "selected_features_kbest = X.columns[feature_selection_kbest.get_support()]\n",
    "selected_features_kbest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_kbest = selected_features_kbest\n",
    "lista_kbest = []\n",
    "for i in method_kbest:\n",
    "    lista_kbest.append(i)\n",
    "lista_kbest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Threshold com os dados entre 0 e 1 — se quiseres StandardScaler – mudar o ‘data_norm’ para ‘data_st’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Threshold\n",
    "feature_selection_variance = VarianceThreshold()\n",
    "feature_selection_variance.fit(data_norm, y)\n",
    "\n",
    "selected_features_variance = X.columns[feature_selection_variance.get_support()]\n",
    "selected_features_variance.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_variance = selected_features_variance\n",
    "lista_variance = []\n",
    "for i in method_variance:\n",
    "    lista_variance.append(i)\n",
    "lista_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select From Model com os dados entre 0 e 1 — se quiseres StandardScaler – mudar o ‘data_norm’ para ‘data_st’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select From Model\n",
    "\n",
    "feature_selection_from_model = SelectFromModel(estimator=LogisticRegression())\n",
    "feature_selection_from_model.fit(data_norm, y)\n",
    "\n",
    "selected_features_from_model = X.columns[feature_selection_from_model.get_support()]\n",
    "selected_features_from_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_from_model = selected_features_from_model\n",
    "\n",
    "lista_from_model = []\n",
    "for i in method_from_model:\n",
    "    lista_from_model.append(i)\n",
    "lista_from_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fazer um consenso entre os métodos para verificar qual será o melhor valor de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_from_model # ver valor da lista\n",
    "lista_variance   # ver valor da lista\n",
    "lista_percentile # ver valor da lista\n",
    "lista_kbest # ver valor da lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare():\n",
    "    z = [ x for x in lista_variance #variance pq da um valor muito elevado\n",
    "          if x in lista_percentile or x in lista_kbest or x in lista_from_model]\n",
    "    print(z)\n",
    "\n",
    "compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_todos = [ x for x in lista_variance\n",
    "          if x in lista_percentile or x in lista_kbest or x in lista_from_model]\n",
    "print(z_todos)\n",
    "len(z_todos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_reduzido = [ x for x in lista_variance\n",
    "          if x in lista_percentile and x in lista_kbest and x in lista_from_model]\n",
    "print(z_reduzido)\n",
    "len(z_reduzido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecionar no dataframe as features que deram no consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset com menos\n",
    "\n",
    "data_reduz_features = df[z_reduzido]\n",
    "data_reduz_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset com mais do consenso\n",
    "\n",
    "data_all_features = df[z_todos]\n",
    "data_all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = [\"green\" if active else \"red\" for active in y]\n",
    "x = len(data_all_features)\n",
    "\n",
    "pca = PCA(n_components = x).fit_transform(data_all_features)\n",
    "xs = principalComponents[:,0]\n",
    "ys = principalComponents[:,1]\n",
    "\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(xs)): axe.plot(xs[j], ys[j], marker=\"o\", color=colors[j], markeredgecolor=\"dimgrey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meti a data_norm que é entre 0 e 1, se for a outra tem de se mudar para data_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.generate_dotplot(data_norm, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded = TSNE(n_components=2,n_iter=1000).fit(data_all_features)\n",
    "fig, axe = plt.subplots(figsize=(20,10))\n",
    "for j in range(len(embeded)): axe.plot(embeded[j,0],embeded[j,1],marker=\"o\",color=colors[j],markeredgecolor=\"dimgrey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlação entre features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividir os dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_reduz_features, y, test_size = 0.25, random_state = 500)\n",
    "\n",
    "\n",
    "print('Train: ', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)\n",
    "\n",
    "print('Train: ', Counter(x for x in y_train))\n",
    "print('Test: ', Counter(x for x in y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressão e curva roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_test_set(model, X_test, y_test):\n",
    "    # score test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "    except:\n",
    "        y_prob = None\n",
    "    print('accuracy', accuracy_score(y_test, y_pred))\n",
    "    print('MCC', matthews_corrcoef(y_test, y_pred))\n",
    "    print('f1 score', f1_score(y_test, y_pred))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    print('fdr', float(fp) / (tp + fp))\n",
    "    print('sn', float(tp) / (tp + fn))\n",
    "    print('sp', float(tn) / (tn + fp))\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test_set(model, X_test, y_test)\n",
    "# ROC_AUC\n",
    "# prediction = model.predict_proba(X_test)\n",
    "\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'g', label='AUC = %0.3f' % roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "print('ROC AUC score:', round(roc_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparar os modelos com e sem pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test and compare multiple models\n",
    "\n",
    "models = [('LR', LogisticRegression()),\n",
    "          ('LR-W', LogisticRegression(class_weight={0: 1, 1: 8})),\n",
    "          ('RF', RandomForestClassifier()),\n",
    "          ('RF-W', RandomForestClassifier(class_weight={0: 1, 1: 8})),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()),\n",
    "          ('SVM-W', SVC(class_weight={0: 1, 1: 8})),\n",
    "          ('DT', DecisionTreeClassifier()),\n",
    "          ('DT-W', DecisionTreeClassifier(class_weight={0: 1, 1: 8}))]\n",
    "\n",
    "\n",
    "names = []\n",
    "results = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    cross_val = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='f1')\n",
    "    results.append(cross_val)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f SD:%f\" % (name, cross_val.mean(), cross_val.std())\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot dos resultados -- all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01,0.1,1],\n",
    "#               'C': [0.1, 1, 10]\n",
    "              'kernel': ['linear']}\n",
    "\n",
    "#hyperparameter optimization algorithm\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=3, refit = True, verbose = 3)\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCORE THE TEST SET\n",
    "score_test_set(grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(abs(grid.best_estimator_.coef_[0]), index=colname).nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão logistica sem peso com RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=0)\n",
    "\n",
    "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
    "\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "clf\n",
    "search = clf.fit(X_train, y_train)\n",
    "search.best_params_\n",
    "\n",
    "score_test_set(search, X_test, y_test)\n",
    "\n",
    "pd.Series(abs(search.best_estimator_.coef_[0]), index=colname).nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest sem pesos com RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [10, 100, 200]}\n",
    "\n",
    "#hyperparameter optimization algorithm\n",
    "grid = RandomizedSearchCV(RandomForestClassifier(), param_grid, cv=3, refit = True, verbose = 3)\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "#SCORE THE TEST SET\n",
    "score_test_set(grid, X_test, y_test)\n",
    "\n",
    "# feature importance for RF\n",
    "pd.Series(abs(grid.best_estimator_.feature_importances_), index=colname).nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas importantes:\n",
    "\n",
    "- fazer o mesmo para o dataset de teste\n",
    "- verificar mais coisas para a parte de ML\n",
    "- decidir que metodo vamos usar para normalizar os dados (MinMaxScaler ou StandardScaler)\n",
    "- verificar o y que me dá erro quando o tento utilizar (não sei pq???)\n",
    "- duvidas = como analisar os dados de tm aka y?\n",
    "- meter como problema binario?? (ex: menor que 50 é 0, maior que 50 é 1) -- não sei se é eficaz ou se faz sentido\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}